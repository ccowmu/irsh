#!/usr/bin/env python3

import sys
import argparse
import requests
from bs4 import BeautifulSoup
import html
import re

def extract_summary(article_elem):
    """Extract a summary from an article element"""
    # Try various common summary locations
    
    # Look for specific summary/excerpt classes first
    for selector in ['p.excerpt', '.post-excerpt', 'p[class*="excerpt"]', 'p[class*="summary"]', 'p[class*="description"]']:
        elem = article_elem.select_one(selector)
        if elem:
            text = elem.get_text(strip=True)
            if text and len(text) > 30:
                return text
    
    # Look for paragraphs with actual content
    paragraphs = article_elem.find_all('p')
    for p in paragraphs:
        text = p.get_text(strip=True)
        # Skip metadata and short text
        if text and len(text) > 30:
            # Skip author/date lines or if it's just the title repeated
            text_lower = text.lower()
            if any(x in text_lower for x in ['by ', 'contributed', 'guest']):
                continue
            # Skip month/year patterns
            if any(x in text for x in [' 202', ' 201']):
                continue
            return text
    
    # If no good paragraph found, return None
    return None

def extract_article_summary(url):
    """Fetch individual article and extract its first paragraph"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for the article content container
        for selector in ['article', '.article-body', '.post-body', 'main']:
            elem = soup.select_one(selector)
            if not elem:
                continue
            
            # Find first substantial paragraph
            for p in elem.find_all('p'):
                text = p.get_text(strip=True)
                if text and len(text) > 30:
                    # Skip paragraphs that look like metadata
                    if not any(x in text.lower() for x in ['by ', 'contributed', 'guest']):
                        return text
        
        return None
    except Exception:
        return None

def fetch_techcrunch_security_news(num=1, sort_order='recent', format_type='brief'):
    """Fetch security news from TechCrunch"""
    url = "https://techcrunch.com/category/security/"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"news: Failed to fetch TechCrunch: {e}", file=sys.stderr)
        sys.exit(1)
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Find articles - look for article containers
    articles = []
    
    # Try to find article containers
    for article_elem in soup.find_all(['article', 'div'], class_=lambda x: x and any(k in x.lower() for k in ['post', 'article', 'story'])):
        # Find the main link (usually the title)
        link_elem = article_elem.find('a', href=True)
        if not link_elem:
            continue
        
        href = link_elem['href']
        
        # Filter out non-article links
        if any(x in href for x in ['#', '?', 'tag/', 'author/', 'page/', 'category']):
            continue
        if 'techcrunch.com' not in href:
            continue
        
        title = link_elem.get_text(strip=True)
        if not title or len(title) < 5:
            continue
        
        articles.append({
            'title': title,
            'link': href,
            'summary': None  # Will fetch later
        })
        
        if len(articles) >= num:
            break
    
    if not articles:
        print("news: No articles found. The site structure may have changed.", file=sys.stderr)
        sys.exit(1)
    
    # Fetch summaries for each article
    print("Fetching article details...\n")
    for article in articles:
        article['summary'] = extract_article_summary(article['link'])
    
    # Print results
    print("=== TechCrunch Security News ===\n")
    
    for i, article in enumerate(articles[:num], 1):
        title = html.unescape(article['title'])
        link = article['link']
        summary = article.get('summary')
        
        if summary:
            summary = html.unescape(summary)
            # Clean up summary text
            summary = re.sub(r'\s+', ' ', summary).strip()
        
        if format_type == 'detailed':
            print(f"[{i}] {title}")
            if summary:
                # For detailed, show much more of the summary
                if len(summary) > 400:
                    print(f"    {summary[:400]}...")
                else:
                    print(f"    {summary}")
            print(f"    Link: {link}\n")
        else:
            print(f"[{i}] {title}")
            if summary:
                # For brief, show shorter summary
                if len(summary) > 100:
                    print(f"    {summary[:100]}...")
                else:
                    print(f"    {summary}")
            print(f"    {link}\n")
    
    print("Done!")

def main():
    parser = argparse.ArgumentParser(
        description='Fetch security news stories from TechCrunch',
        prog='news'
    )
    parser.add_argument('-n', '--num', type=int, default=1, 
                        help='Number of stories to fetch (1-50, default: 1)')
    parser.add_argument('-s', '--sort', choices=['recent', 'older'], default='recent',
                        help="Sort order: 'recent' or 'older' (default: recent)")
    parser.add_argument('-f', '--format', choices=['brief', 'detailed'], default='brief',
                        help="Output format: 'brief' or 'detailed' (default: brief)")
    
    args = parser.parse_args()
    
    # Validate num
    if args.num < 1 or args.num > 50:
        print("news: Invalid number of stories (must be 1-50)", file=sys.stderr)
        sys.exit(1)
    
    print(f"Fetching {args.num} {args.sort} security stories from TechCrunch...\n")
    
    fetch_techcrunch_security_news(args.num, args.sort, args.format)

if __name__ == '__main__':
    main()
